{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, welcome to Pytorch 101.\n",
    "\n",
    "What this notebook and session intends to cover:\n",
    "1. How to deal with your data\n",
    "2. How to work with pytorch tensors, and make use of the gpu\n",
    "3. How to set up a basic model\n",
    "4. How to use loss functions and optimisers\n",
    "5. How to set up a basic training loop\n",
    "\n",
    "What this notebook and session will not cover:\n",
    "1. How to work with custom architectures\n",
    "2. How the actual mathematics of autograd works (I'll go over a basic version of it though)\n",
    "3. How to deal with checkpoints\n",
    "\n",
    "I'll can cover all this in a second part if you guys are fine with it and want it.\n",
    "\n",
    "Prerequisites: Python and numpy knowledge, ml/dl basic knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin with our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good place to start, let us now pull in our csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 3.9690e+02, 4.9800e+00,\n",
       "        2.4000e+01],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 3.9690e+02, 9.1400e+00,\n",
       "        2.1600e+01],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 5.6400e+00,\n",
       "        2.3900e+01],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 3.9345e+02, 6.4800e+00,\n",
       "        2.2000e+01],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 3.9690e+02, 7.8800e+00,\n",
       "        1.1900e+01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.genfromtxt(\"BostonHousing.csv\", delimiter=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I prefer calling in via numpy, you can use pandas or something else, it really doesn't matter too much\n",
    "\n",
    "Looking at the top row, we can see they aren't numbers, and they are the headings, so we can get rid of that.\n",
    "\n",
    "Looking at the way the data is structured on the competition page, the last coloumn is the output we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of headings\n",
    "data=np.delete(data,0,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7310e-02, 0.0000e+00, 7.0700e+00,        nan, 4.6900e-01,\n",
       "       6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "       1.7800e+01, 3.9690e+02, 9.1400e+00, 2.1600e+01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at one row of the data\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the fourth coloumn here is a string and a dummy variable as per the competition, so let's get rid of that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of the river bank datapoint cuz idk wtf to do with it\n",
    "data=np.delete(data,3,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7310e-02, 0.0000e+00, 7.0700e+00, 4.6900e-01, 6.4210e+00,\n",
       "       7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n",
       "       3.9690e+02, 9.1400e+00, 2.1600e+01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at one row of the data\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have a bunch of numbers now! What next? Let us sort out our input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what does our data look like?\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, once we have our data ready, we should convert to torch tensors for further processing. And for the ones over here who might have worked with cuda before, and are wondering why we haven't sent our data over to the device yet; it is for memory optimisation reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([506, 12]), torch.Size([506, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting it into x and y\n",
    "x=data[::,:12]\n",
    "y=data[::,12:]\n",
    "x=torch.Tensor(x).float()\n",
    "y=torch.Tensor(y).float()\n",
    "#then confirming the shapes of those\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have an input and an output. Let us now split it into a train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to split it into train and validation parts\n",
    "#sorting out our indexes\n",
    "idx=np.arange(506)\n",
    "#shuffling it\n",
    "np.random.shuffle(idx)\n",
    "#putting the corresponding parts into train and test indexes\n",
    "train_idx, test_idx=idx[:450],idx[450:]\n",
    "#and finally splitting our our shuffled data\n",
    "x_train, y_train=x[train_idx],y[train_idx]\n",
    "x_test, y_test=x[test_idx],y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be made a whole lot simpler with the Dataloader package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need a bunch of imports for this\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "#you can simply call it from torch itself too, but this \n",
    "#is slightly more convinient and legible with the actual code\n",
    "dataset= TensorDataset(x,y)\n",
    "train, val= random_split(dataset,[450, 56])\n",
    "batch_size=20\n",
    "train_dl=DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_dl=DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done essentially, is set up the pairing of data here, and it is very useful later on when we will sort out our training loop. We can unpack train_dl to x and y during training in batches, and simplifies the process of splitting up the data for us too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us set up our gpu device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant, that seems to have worked very well. Now everytime we want something to be run on the gpu, we will simply send it over to the gpu via device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, our model. For this session, I have kept things super simple by using a normal Linear model. We will call it from pytorch itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Linear(in_features=12, out_features=1, bias=True)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1718,  0.0935, -0.2790, -0.1299, -0.0726, -0.1178, -0.1808,  0.0628,\n",
       "          -0.2526,  0.2728,  0.0803,  0.0480]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0584], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "model= nn.Linear(12,1).to(device)\n",
    "print(model.parameters)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of things happening here, so lets break it down.\n",
    "\n",
    "We first called in nn.Linear, and passed two parameters to it, the first one is number of input parameters, and the second one is number of output parameters. We then sent it over to \"device\", device here being our gpu.\n",
    "We then saw the general structure of the model, and even saw what the exact random initialisations of the model looks like.\n",
    "\n",
    "There is a line in there which says requires_grad=True for both the weights and the biases. Keep that in the back of your mind, it is super useful later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us set up our Loss function. For this exercise, we are limited to using RMSE, so we will call in mse from the functions of torch, and then apply a square root to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def loss_fn(inp,tar):\n",
    "    loss = F.mse_loss(inp,tar)\n",
    "    RMSE_loss=loss.sqrt()\n",
    "    return RMSE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(109.7696, device='cuda:0', grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check of the loss function\n",
    "loss_fn(model(x_train.to(device)),y_train.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So pretty simple, we called in mse loss, from torch's functional package, then sqaure rooted the answer and returned it. When we ran it, we had to send over the data in the form of torch tensors which were on the gpu, since our model is on the gpu as well. Not doing so will cause conflicts and weird ass errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us set up our optimiser. For this one, we are using the stochastic gradient descent optimiser. For those of you familiar with how SGD works, while I might not be having specific code to sort my data into minibatches, it is happening in the back end of the optimiser automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-7\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we have moved all our pieces into play, we can now start with out training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up our training loop\n",
    "import copy\n",
    "best_model=model\n",
    "def train(epoch_count, model, loss_method, optimizer=optimizer, train_dl=train_dl):\n",
    "    lowest_loss=None\n",
    "    for epoch in range (epoch_count):\n",
    "        for xb, yb in train_dl:\n",
    "            xb=xb.to(device)\n",
    "            yb=yb.to(device)\n",
    "            hyp=model(xb)\n",
    "            loss=loss_method(hyp, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if (epoch==0):\n",
    "            lowest_loss=loss.item()\n",
    "        if ((epoch+1)%100==0):\n",
    "            print(f\"Epoch number:{epoch+1}: Loss={loss.item():.2f}\")\n",
    "        if (lowest_loss>loss.item()):\n",
    "            lowest_loss=loss.item()\n",
    "            best_model=copy.deepcopy(model)\n",
    "    print(f\"Best model error={lowest_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:100: Loss=54.71\n",
      "Epoch number:200: Loss=26.25\n",
      "Epoch number:300: Loss=12.64\n",
      "Epoch number:400: Loss=16.01\n",
      "Epoch number:500: Loss=7.82\n",
      "Epoch number:600: Loss=11.66\n",
      "Epoch number:700: Loss=13.14\n",
      "Epoch number:800: Loss=6.04\n",
      "Epoch number:900: Loss=10.62\n",
      "Epoch number:1000: Loss=9.91\n",
      "Best model error=2.3848319053649902\n"
     ]
    }
   ],
   "source": [
    "train(1000, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a ton of things happening here again, let's break it down.\n",
    "\n",
    "I first called in an import which would help us extract our best model, we'll come to that later.\n",
    "\n",
    "I then started my training loop based on the number of epochs I decided to pass to it, in this case, 1000, and started unpacking my data into batches of xb and yb. If we go back and see, I set my batch size as 20, which means if there is a set where there aren't 20 rows available, then it will take as many as there are left, and then move on to the next epoch. \n",
    "\n",
    "Then I sent over my batch data to my gpu, this way I am not cluttering it with the entirity of my data at one go. This is super useful when dealing with hundreds of gigabytes of data at once.\n",
    "\n",
    "Then I set up my hypothesis, passed it through the loss function, and ran loss.backwards. Now this is one of the many places where pytorch really shines. What loss.backwards is doing, is looking through every equation from the loss equation, all the way to the original linear equation which is our model. It will compute the gradients of every parameter which has requires_grad set to True. And this is super helpful for the next step.\n",
    "\n",
    "When I run optimizer.step(), we essentially update each and every parameter. If we go back to where I defined the optimizer, we can see one of the parameters for it is the parameters of the model (note, former parameter here is the programming one, latter is the parameter of the model).\n",
    "\n",
    "Finally, I run optimizer.zero_grad(). This is because I want to set all the gradients to zero for the next time it is computed. Pytorch normally has them accumalate to make writing RNNs simpler, and so for any other case, we set them to zero every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit handles the saving of the best model. I noticed while training that the loss converges a decent amount, then keeps jumping around. So I wanted to save the model with the lowest loss. I essentially deepcopied the model to best_model whenever the current loss was lower than the best loss recorded till then, initialising best_loss with the first epoch's loss. The deepcopying is necessary, otherwise simply setting it to equal will act as a pointer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3726, device='cuda:0', grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(best_model(x_test.to(device)),y_test.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give this a quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.8435], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs=best_model(x_test.to(device))\n",
    "outputs[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.1000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for today folks! \n",
    "\n",
    "*If you liked it, make sure to like, share, subscribe, hit notification bell, forward, put as story, tag, eat, shit and breathe pytorch from now on*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
